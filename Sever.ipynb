{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNw73CvOevs8LsKCPlJ5QdG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","UCF Crime Detection - Video Processing and Inference\n","Author: matapv01\n","Created: 2025-03-15 09:27:27\n","\"\"\"\n","import os\n","import cv2\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","from PIL import Image\n","import torch.nn as nn\n","from datetime import datetime\n","from torchvision import transforms\n","from transformers import ViTImageProcessor, ViTForImageClassification\n","from huggingface_hub import login\n","from IPython.display import HTML\n","from base64 import b64encode\n","from google.colab import files\n","# Install required packages\n","!pip install transformers torch torchvision opencv-python pillow tqdm\n","import cv2\n","from IPython.display import HTML\n","from base64 import b64encode\n","from transformers import ViTForImageClassification, ViTImageProcessor\n","import torch\n","# Load model\n","# Import the necessary modules\n","from huggingface_hub import login, HfApi\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"S8lUOCEmVwKN","executionInfo":{"status":"ok","timestamp":1742481130584,"user_tz":-420,"elapsed":16686,"user":{"displayName":"Minh Phạm Văn","userId":"09034337405395880200"}},"outputId":"9e69797a-e83f-4f32-c53b-99a7eebef175"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"]}]},{"cell_type":"code","source":["# Hugging Face settings\n","MODEL_ID_2 = \"mata01/crime-20frame-detection-vit-model\"  # Your model repo name\n","#HF_TOKEN = \"hf_xxx\"  # Replace with your token\n","from google.colab import userdata\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","token = HF_TOKEN\n","\n","if HF_TOKEN:\n","  print(f\"Loaded token success\")\n"],"metadata":{"id":"sWgou6up40TJ","executionInfo":{"status":"ok","timestamp":1742481134549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Minh Phạm Văn","userId":"09034337405395880200"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1fa7089-3459-4f9c-9623-2d2c2f064577"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded token success\n"]}]},{"cell_type":"code","source":["# Constants\n","FRAME_CHUNK_SIZE = 20\n","BATCH_SIZE = 32\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Label mapping\n","LABELS = {\n","    0: \"Abuse\", 1: \"Arrest\", 2: \"Arson\", 3: \"Assault\",\n","    4: \"Burglary\", 5: \"Explosion\", 6: \"Fighting\", 7: \"Normal\",\n","    8: \"Road Accident\", 9: \"Robbery\", 10: \"Shooting\",\n","    11: \"Shoplifting\", 12: \"Stealing\", 13: \"Vandalism\"\n","}\n","\n","class VideoProcessor:\n","    def __init__(self):\n","        print(\"Loading model from Hugging Face...\")\n","        # Initialize video properties\n","        self.fps = None\n","        self.frame_width = None\n","        self.frame_height = None\n","\n","        try:\n","            # Login to Hugging Face\n","            login(HF_TOKEN)\n","\n","            # Load model and processor\n","            self.model = ViTForImageClassification.from_pretrained(\n","                MODEL_ID_2,\n","                use_auth_token=HF_TOKEN,\n","                num_labels=len(LABELS),\n","                id2label=LABELS\n","            ).to(DEVICE)\n","\n","            self.processor = ViTImageProcessor.from_pretrained(\n","                MODEL_ID_2,\n","                use_auth_token=HF_TOKEN\n","            )\n","\n","            print(\"Model loaded successfully!\")\n","\n","        except Exception as e:\n","            print(f\"Error loading from Hugging Face: {str(e)}\")\n","            raise\n","\n","        self.model.eval()\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize(\n","                mean=[0.485, 0.456, 0.406],\n","                std=[0.229, 0.224, 0.225]\n","            )\n","        ])\n","\n","    def extract_frames(self, video_path):\n","        print(\"Extracting frames from video...\")\n","        frames = []\n","        cap = cv2.VideoCapture(video_path)\n","\n","        if not cap.isOpened():\n","            raise ValueError(f\"Could not open video file: {video_path}\")\n","\n","        # Get video properties\n","        self.fps = int(cap.get(cv2.CAP_PROP_FPS))\n","        self.frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        self.frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","        print(f\"Video properties: {self.fps} FPS, {self.frame_width}x{self.frame_height}\")\n","\n","        for _ in tqdm(range(total_frames), desc=\"Extracting frames\"):\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frames.append(frame)\n","\n","        cap.release()\n","        return frames\n","\n","    def create_frame_chunks(self, frames):\n","        print(\"Creating frame chunks...\")\n","        chunks = []\n","        for i in range(0, len(frames), FRAME_CHUNK_SIZE):\n","            chunk = frames[i:i + FRAME_CHUNK_SIZE]\n","            if len(chunk) == FRAME_CHUNK_SIZE:  # Only use complete chunks\n","                chunks.append(chunk)\n","        return chunks\n","\n","    def process_chunk(self, chunk):\n","        processed_frames = []\n","        for frame in chunk:\n","            pil_image = Image.fromarray(frame)\n","            processed_frame = self.transform(pil_image)\n","            processed_frames.append(processed_frame)\n","\n","        batch = torch.stack(processed_frames).to(DEVICE)\n","\n","        with torch.no_grad():\n","            outputs = self.model(batch)\n","            predictions = torch.softmax(outputs.logits, dim=-1)\n","            chunk_pred = predictions.mean(dim=0)\n","            label_idx = chunk_pred.argmax().item()\n","            confidence = chunk_pred[label_idx].item()\n","\n","        return LABELS[label_idx], confidence\n","\n","    def create_output_video(self, frames, predictions, output_path):\n","        print(\"Creating output video...\")\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_path, fourcc, self.fps,\n","                            (self.frame_width, self.frame_height))\n","\n","        # Create mapping from frame index to prediction\n","        frame_predictions = {}\n","        for chunk_idx, (label, conf) in enumerate(predictions):\n","            start_frame = chunk_idx * FRAME_CHUNK_SIZE\n","            end_frame = start_frame + FRAME_CHUNK_SIZE\n","            for frame_idx in range(start_frame, end_frame):\n","                frame_predictions[frame_idx] = (label, conf)\n","\n","        print(\"Adding labels to frames...\")\n","        for frame_idx, frame in enumerate(tqdm(frames, desc=\"Processing frames\")):\n","            if frame_idx in frame_predictions:\n","                label, conf = frame_predictions[frame_idx]\n","\n","                # Create copy of frame\n","                frame = frame.copy()\n","\n","                # Text to display\n","                text = f\"Action: {label} ({conf:.2f})\"\n","\n","                # Draw black background\n","                (text_width, text_height), _ = cv2.getTextSize(\n","                    text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2\n","                )\n","                cv2.rectangle(\n","                    frame,\n","                    (10, 10),\n","                    (text_width + 20, 40),\n","                    (0, 0, 0),\n","                    -1\n","                )\n","\n","                # Draw white text\n","                cv2.putText(\n","                    frame,\n","                    text,\n","                    (15, 30),\n","                    cv2.FONT_HERSHEY_SIMPLEX,\n","                    1,\n","                    (255, 255, 255),\n","                    2,\n","                    cv2.LINE_AA\n","                )\n","\n","            # Write frame\n","            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","            out.write(frame_bgr)\n","\n","        out.release()\n","        print(f\"Video saved to {output_path}\")\n","\n","    def process_video(self, input_path, output_path = 'static/outputs/video.mp4'):\n","        try:\n","            # Extract frames\n","            frames = self.extract_frames(input_path)\n","            if not frames:\n","                raise ValueError(\"No frames extracted from video\")\n","\n","            print(f\"Total frames: {len(frames)}\")\n","\n","            # Create and process chunks\n","            chunks = self.create_frame_chunks(frames)\n","            print(f\"Total chunks: {len(chunks)}\")\n","\n","            # Process chunks\n","            print(\"Processing chunks...\")\n","            predictions = []\n","            for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n","                label, confidence = self.process_chunk(chunk)\n","                predictions.append((label, confidence))\n","\n","            # Create output video\n","            print(\"\\nCreating output video...\")\n","            self.create_output_video(frames, predictions, output_path)\n","\n","            return output_path\n","\n","        except Exception as e:\n","            print(f\"Error processing video: {str(e)}\")\n","            return\n","\n","def show_video(video_path):\n","    \"\"\"\n","    Displays a video in the notebook\n","    \"\"\"\n","    mp4 = open(video_path, 'rb').read()\n","    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","    return HTML(f\"\"\"\n","    <video width=600 controls>\n","        <source src=\"{data_url}\" type=\"video/mp4\">\n","    </video>\n","    \"\"\")"],"metadata":{"id":"8W0ZOe2aV6jj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loadimgprocessor():\n","  # Định nghĩa repository ID\n","  username = \"mata01\"  # Thay bằng username thật của bạn\n","  model_name = \"crime_action_cctv_image_detection\"\n","  repo_id = f\"{username}/{model_name}\"\n","\n","  # Tải mô hình và tiền xử lý từ Hugging Face Model Hub\n","  model = ViTForImageClassification.from_pretrained(repo_id, token=token)\n","  processor = ViTImageProcessor.from_pretrained(repo_id, token=token)\n","\n","  # 1. Kiểm tra và chọn thiết bị (GPU nếu có)\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  print(f\"Using device: {device}\")\n","\n","  # Chuyển mô hình sang GPU\n","  model.to(device)\n","  model.eval()  # Đặt mô hình ở chế độ evaluation để tối ưu suy luận\n","  print(\"Model moved to GPU successfully!\")\n","  return model, processor\n","\n","from PIL import Image, ImageDraw, ImageFont\n","import os\n","\n","def predict_image(image_path, output_path='static/outputs/output_image.jpg'):\n","    try:\n","        # Đọc ảnh sử dụng OpenCV\n","        img = cv2.imread(image_path)\n","        if img is None:\n","            print(f\"Lỗi: Không thể đọc ảnh tại {image_path}\")\n","            return\n","\n","        # Chuyển đổi ảnh sang RGB\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","        # Tiền xử lý ảnh\n","        inputs = processor_img(images=img, return_tensors=\"pt\").to(device)\n","\n","        # Dự đoán\n","        with torch.no_grad():\n","            outputs = model_img(**inputs)\n","            logits = outputs.logits\n","            predicted_class_id = logits.argmax().item()\n","            predicted_label = model_img.config.id2label[predicted_class_id]\n","\n","        # Hiển thị ảnh với nhãn dự đoán\n","        pil_img = Image.fromarray(img)\n","        draw = ImageDraw.Draw(pil_img)\n","        font_size = 30\n","        font = ImageFont.truetype(\"LiberationSansNarrow-Bold.ttf\", font_size)\n","        text_position = (10, 10)\n","        text_color = (255, 0, 0)\n","\n","        # Vẽ nhãn dự đoán lên ảnh\n","        draw.text(text_position, predicted_label, fill=text_color, font=font)\n","\n","        # Lưu ảnh đầu ra\n","        pil_img.save(output_path)\n","        return output_path\n","\n","    except Exception as e:\n","        import traceback\n","        traceback.print_exc()\n","        print(f\"Đã xảy ra lỗi: {e}\")\n","        return"],"metadata":{"id":"BUcSC18YWDXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muaCduZ9VhN_","executionInfo":{"status":"ok","timestamp":1742481533307,"user_tz":-420,"elapsed":6021,"user":{"displayName":"Minh Phạm Văn","userId":"09034337405395880200"}},"outputId":"ed791856-2160-454e-b760-23ddecf54681"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading models...\n","Loading model from Hugging Face...\n","Model loaded successfully!\n","Using device: cpu\n","Model moved to GPU successfully!\n","Models loaded successfully!\n"]}],"source":["# Khởi tạo các model AI\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Loading models...\")\n","processor = VideoProcessor()\n","model_img, processor_img = loadimgprocessor()\n","print(\"Models loaded successfully!\")"]},{"cell_type":"code","source":["# predict_image('/content/frame_01123.jpg')"],"metadata":{"id":"CRuOPCAj-W0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# output_video_path = '/content/output_video.mp4'\n","# input_video_path = '/content/Burglary at Colorado business caught on camera.mp4'\n","# processor.process_video(input_video_path, output_video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6CYMMNP_KIr","executionInfo":{"status":"ok","timestamp":1742482057720,"user_tz":-420,"elapsed":511378,"user":{"displayName":"Minh Phạm Văn","userId":"09034337405395880200"}},"outputId":"43a1b9f4-da8f-4f69-fb8e-f1b5124330f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting frames from video...\n","Video properties: 29 FPS, 1920x1080\n"]},{"output_type":"stream","name":"stderr","text":["Extracting frames: 100%|██████████| 1050/1050 [00:16<00:00, 63.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total frames: 1050\n","Creating frame chunks...\n","Total chunks: 52\n","Processing chunks...\n"]},{"output_type":"stream","name":"stderr","text":["Processing chunks: 100%|██████████| 52/52 [08:02<00:00,  9.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Creating output video...\n","Creating output video...\n","Adding labels to frames...\n"]},{"output_type":"stream","name":"stderr","text":["Processing frames: 100%|██████████| 1050/1050 [00:11<00:00, 90.32it/s] "]},{"output_type":"stream","name":"stdout","text":["Video saved to /content/output_video.mp4\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":[],"metadata":{"id":"PAGSectGQC2C"},"execution_count":null,"outputs":[]}]}